**SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#4 â€“ Mutation Testing and Web app testing**

| Group: 3                  |
|---------------------------|
| Student 1 Ahmed Abbas     |   
| Student 2 Rimal Rizvi     |   
| Student 3 Mariyah Malik   |   
| Student 4 Macayla Konig   |   

# Introduction

In this lab we focused on mutation testing and GUI testing. We learned how to inject mutation faults in Java code base using a mutation testing tool and how to interpret the reported mutation scores and used our knowledge to design new test cases to improve the overall quality of the test suite. We also learned how to use Selenium - a well known tool for web interface testing and compared it to another tool, Sikulix. We also learned how to design test cases for a web application and how to use assertions and checkpoints to validate the behavior and state of web applications during test execution.


# Analysis of 10 Mutants of the Range class

Based on the Pitest report, we can analyze at least 10 mutants produced for the Range class and how they are handled by the original test suite:

1. AOR3Mutator: This mutator introduces changes to arithmetic operations by replacing the third argument of a binary arithmetic operator with zero. However, all 8 mutants generated by this mutator were killed by the test suite. This indicates that the test suite is effective in detecting changes in arithmetic operations.

2. AOR2Mutator: Similar to AOR3Mutator, AOR2Mutator modifies binary arithmetic operations by replacing the second argument with a different value. Again, all 8 mutants were killed, showing that the test suite adequately tests arithmetic operations.

3. VoidMethodCallMutator: This mutator replaces method calls with void return types with a method call to another method with a void return type. The test suite didn't kill any mutants generated by this mutator, indicating a potential gap in testing void method calls.
4. AOR1Mutator: AOR1Mutator modifies binary arithmetic operations by replacing the first argument with a different value. All 8 mutants were killed, demonstrating that the test suite effectively handles changes to arithmetic operations.
5. OBBN3Mutator: This mutator generates mutants by replacing boolean operators (&& and ||) with their logical equivalents. However, none of the mutants generated by this mutator were killed by the test suite, suggesting a potential weakness in testing boolean operators.
6. AOR4Mutator: AOR4Mutator replaces arithmetic operations by dividing or multiplying the first operand by 1. Only 7 out of 8 mutants were killed, indicating that there might be some cases where the test suite doesn't adequately cover arithmetic operations.
7. SwitchMutator: This mutator generates mutants by removing switch statements. None of the mutants generated by this mutator were killed by the test suite, indicating a potential gap in testing switch statements.
8. RemoveConditionalMutator_EQUAL_ELSE: This mutator generates mutants by removing else conditions in conditional statements. Only 1 out of 4 mutants was killed, suggesting that the test suite might not adequately cover cases with else conditions.
9. MathMutator: MathMutator generates mutants by replacing arithmetic expressions with their absolute values. All 8 mutants generated by this mutator were killed, indicating effective testing of arithmetic expressions.
10. OBBN1Mutator: Similar to OBBN3Mutator, OBBN1Mutator generates mutants by changing boolean operators. However, none of the mutants generated by this mutator were killed by the test suite, indicating a potential weakness in testing boolean operators.

Overall, the analysis shows that the test suite is quite effective in killing mutants related to arithmetic operations but may have some weaknesses in handling mutants related to void method calls, boolean operators, switch statements, and certain conditional statements. Further enhancements to the test suite could improve mutation testing coverage in these areas.

# Report all the statistics and the mutation score for each test class



## DataUtilitiesTest

```
- Statistics
================================================================================
>> Line Coverage: 112/57263 (0%)
>> Generated 358697 mutations Killed 752 (0%)
>> Mutations with no coverage 357801. Test strength 84%
>> Ran 20022 tests (0.06 tests per mutation)
```
Mutation Score: 81%

## RangeTest

```
- Statistics
================================================================================
>> Line Coverage: 27/57261 (0%)
>> Generated 358700 mutations Killed 268 (0%)
>> Mutations with no coverage 358378. Test strength 83%
>> Ran 3584 tests (0.01 tests per mutation)
```

# Analysis drawn on the effectiveness of each of the test classes

The DataUtilitiesTest class is shown to be effective when considering metrics such as Test Strength (87%) and Mutation Coverage (81%). The Test Strength measures how much of the code is exercised by the test suite. A high Test Strength indicates that a large portion of the code is being tested - a crucial component in determining the reliability and robustness of a test suite. A high Mutation Coverage indicates that the test suite is effective in catching changes within the code, thus providing evidence of the test suite's ability to detect potential bugs. Having high values for both Test Strength and Mutation Coverage are positive indications which suggest that the tests for the DataUtilities class are comprehensive in covering the code and effective in detecting potential defects.

The RangeTest class demonstrates limited effectiveness in testing the Range class, as evidenced by its 0% line coverage and mutation coverage. Despite achieving a test strength of 83%, indicating a comprehensive testing effort, the absence of line coverage suggests that the test suite fails to execute any lines of code within the Range class. Additionally, the low mutation coverage indicates that the test suite struggles to detect changes or defects in the Range class code, with only a small fraction of mutations being killed by the tests. To improve the effectiveness of the RangeTest class, it is crucial to augment the test suite with additional test cases covering various scenarios and to refine the test design to achieve higher line and mutation coverage, thereby enhancing its ability to detect potential bugs and ensure the reliability of the Range class.


# A discussion on the effect of equivalent mutants on mutation score accuracy

To automatically detect equivalent mutants, we can leverage the mutation testing tool's logs and analyze the generated mutation score for each test case. Here's a possible approach:

1. Run the mutation testing tool on the classes Range and DataUtilities.
2. Collect the mutation logs generated by the tool.
3. Analyze the mutation logs to identify mutants that have the same status (e.g., killed or survived) across all test cases.
4. Compare the mutated code of these mutants to identify any similarities or patterns.
5. If mutants have the same status and similar mutated code, they can be considered as potential equivalent mutants.

Benefits of this approach:
- Automation: The process can be automated, saving time and effort in manually analyzing each mutant.
- Scalability: It can be applied to large codebases with numerous mutants and test cases.
- Consistency: The detection process is consistent and objective, reducing the chances of human error.

Disadvantages and Assumptions:
- False Positives: There is a possibility of incorrectly identifying mutants as equivalent due to similarities in the mutated code that are not functionally equivalent.
- Limited Scope: This approach only considers mutants with the same status across all test cases, potentially missing mutants that are functionally equivalent but have different statuses in specific test cases.
- Code Coverage: The effectiveness of this approach relies on having sufficient test coverage to detect mutants and differentiate between equivalent and non-equivalent mutants.

Manual Detection of Equivalent Mutants:
To manually detect equivalent mutants, follow these steps:

1. Review the mutation logs generated by the tool.
2. Identify mutants that have the same status (e.g., killed or survived) across all test cases.
3. Analyze the mutated code of these mutants to identify any similarities or patterns.
4. Compare the behavior of the original code and the equivalent mutants to determine if they produce the same output or behavior.


# A discussion of what could have been done to improve the mutation score of the test suites

To improve the mutation score of the test suites, the following strategies could be considered:

1. Increase Test Coverage: Identify areas of the codebase with low test coverage and create additional test cases to exercise those areas. This can help detect more mutants and improve the mutation score.

2. Diversify Test Data: Use a variety of test data to cover different scenarios and edge cases. This can help identify mutants that are not caught by existing test cases and improve the mutation score.

3. Refactor Test Cases: Review and refactor existing test cases to ensure they are effective at detecting mutants. This may involve improving assertions, adding more specific test conditions, or removing redundant test cases.

4. Prioritize Mutants: Analyze the mutation logs to identify mutants that are difficult to detect and prioritize them for additional testing. This can help focus efforts on mutants that have a significant impact on the mutation score.

5. Leverage Mutation Testing Tools: Use mutation testing tools to identify equivalent mutants and remove them from the analysis. This can help improve the accuracy of the mutation score by focusing on non-equivalent mutants.

6. Collaborate with Developers: Work closely with developers to understand the codebase and identify areas that are prone to defects. This can help prioritize testing efforts and improve the mutation score by targeting high-risk areas.

7. Continuous Improvement: Implement a process for continuous improvement of the test suites, including regular reviews, refactoring, and updating test cases based on changes in the codebase.

By implementing these strategies, the mutation score of the test suites can be improved, leading to more effective detection of mutants and better overall test coverage.


# Why do we need mutation testing? Advantages and disadvantages of mutation testing

Mutation testing is a technique used in software testing to evaluate the quality of test suites. It is needed for evaluating the effectiveness of test suites by using small changes or mutations in the code and checking to see if the tests can detect them. It identifies weaknesses in test coverage, encourages better quality of the test suite and increases the reliability of the code by ensuring that potential faults are thoroughly checked.

Advantages:
- Evaluates the effectiveness of test suites by measuring its ability to detect changes (mutations) in the source code. It provides a quantitative measure of how well the tests can identify faults in the code
- Mutation testing identifies weaknesses in the test suite by using artificial faults (mutations) in the code and checking if the tests can detect them. It shows the areas of the code that are not properly covered by the tests
- Helps improve test quality by identifying weaknesses in the test suite so you are able to write more thorough tests that improve coverage 
- Ensures the correctness of the codebase by ensuring that the tests are thorough enough to detect bugs 
- Encourages regular maintenance of test suites since it provides immediate feedback on the effectiveness of tests

Disadvantages:
- Can increase testing time and resource consumption significantly when testing large codebases with extensive test suites
- Mutation testing may produce false positives (incorrectly passing mutants) or false negatives ( undetected mutants). This cause misrepresentation of the test suite
- The effectiveness of mutation testing relies heavily on the expected outcomes/oracles, so if the oracles are inaccurate or incomplete, the results of mutation testing can be misleading and unreliable
- It is difficult to generate meaningful mutants that represent faults in the code


# Explain your SELENUIM test case design process

[Samsung Page](www.samsung.com/ca/)


To begin to design Selenium test cases for the Samsung website, we must first identify functionalities of the application to be tested. Then breaking these functionalities into test scenarios which represent a specific flow or behviour that we will test. For each test scenario, we will define individual test cases that cover different aspects of the functionality. We can then create test data sets to cover various scenarios and conditions by using different input data for each test case, including valid and invalid data, edge cases and boundary conditions. We will then choose the appropriate Selenium commands and methods to perform actions on the applications UI. Some commands we may use are locating elements, clicking buttons, entering text, submitting forms, etc. Next, we will write test scripts in Java using the appropriate assertions and checkpoints. Assertions and checkpoints are used to ensure the behaviour of the application is correct (actual vs expected results). 


# Explain the use of assertions and checkpoints
Assertions and checkpoints are essential components of test automation frameworks, including Selenium, to validate the behavior and state of web applications during test execution. 

Assertions:
- Assertions are used to verify the expected behavior and outcomes of test scenarios by comparing the actual results with the expected results.
- In Selenium test cases, assertions are used to validate the presence, visibility, text content, and attributes of web elements, such as buttons, input fields, dropdowns, and links.

Checkpoints:
- Checkpoints are used to validate the state and behavior of web elements and application components during test execution.
- In Selenium test cases, checkpoints are used to verify the expected state of web elements, such as their visibility, text content, and attributes, at specific points in the test scenario.

# how did you test each functionaity with different test data


# Discuss advantages and disadvantages of Selenium vs. Sikulix

## Selenium

Advantages:
- Supports multiple browsers such as Chrome and Safari, making it suitable for testing web applications across different browsers
- Supports multiple programming languages such as Java, Python, C#
- It is easily integrated with testing frameworks such as JUnit
- Provides robust mechanisms for locating elements on a web page using locators

Disadvantages:
- It is limited to web application testing and does not support desktop or mobile applications
- It struggles with non-HTML elements like pop-up windows 

## Sikulix

Advantages:
- Uses image recognition to automate GUIs which makes it suitable for desktop apps, certain web apps with non-HTML components, and virtual environments
- Cross-platform - works on Windows, macOS and Linux without big usage differences
- Easy to use - relies heavily on visual recognition rather than complex coding

Disadvantages:
- Slower performance compared to Selenium, especially when dealing with large or complex GUIs
- Scripts rely entirely on visual appearance of elements, making them more susceptible to changes in UI design or resolution differences
- Capabilities for web automation are not as robust or flexible as Selenium
- Debugging Sikulix scripts can be more challenging compared to Selenium scripts, especially when dealing with issues related to image recognition or identifying elements 



# How the team work/effort was divided and managed

The team divided the work based on individual strengths and expertise. Ahmed  and Rimal focused on mutation testing analysis and test case design, leveraging their knowledge of Java and testing principles. Mariyah and Macayla took charge of the Selenium and Sikulix testing aspects, drawing on their experience with web development and GUI testing tools. Regular meetings were held to discuss progress, share insights, and address any challenges encountered during the lab. Each team member contributed effectively to their assigned tasks, ensuring a comprehensive and well-rounded approach to the lab objectives.

# Difficulties encountered, challenges overcome, and lessons learned

The team encountered challenges in interpreting the mutation testing results and identifying areas for improvement in the test suites. This required a deep understanding of the mutation testing process and the ability to analyze the generated mutation scores effectively. The team overcame this challenge by conducting thorough research, seeking guidance from the lab materials, and collaborating to share insights and perspectives. This experience highlighted the importance of effective communication and collaboration in addressing complex testing challenges.

# Comments/feedback on the lab itself

Overall, the lab provided valuable insights into mutation testing techniques, GUI testing tools, and test case design strategies for web applications. The hands-on exercises and practical assignments facilitated a deeper understanding of software testing concepts and methodologies, preparing the team members for real-world testing scenarios. However, there were areas where additional guidance or resources could have been beneficial, particularly in understanding complex mutation testing concepts and advanced features of Selenium and Sikulix. Nevertheless, the lab proved to be a valuable learning experience, equipping the team with essential skills and knowledge in software testing, reliability, and quality assurance.
